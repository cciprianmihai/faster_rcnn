{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LL4ujGNBhtpV"
      },
      "source": [
        "EDIT THE FIRST CELL TO CUSTOMIZE YOUR ENVIRONMENT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QwXBRdMBhoQE"
      },
      "outputs": [],
      "source": [
        "use_colab=True  # True daca folosesti colab\n",
        "resume_training = True  # True daca vrei sa continui antrenarea unui model deja antrenat. Poti lasa True din prima pentru ca am inglobat si cazul daca nu exista un checkpoint\n",
        "data_dir = \"/content/data\"  # directorul in care se afla datele de input\n",
        "ckpt_dir = \"/content/drive/MyDrive/Colab Notebooks/Dissertation Results/Checkpoints/\"  # directorul in care se salveaza checkpoint-urile\n",
        "utils_dir = \"/content/drive/My Drive/Colab Notebooks/dissertation_resources\"  # directorul in care se afla utilitarele (in zip este utils)\n",
        "dataset_names = ['brats']  # numele dataset-urilor pe care le folosesti, sunt ordonate [source, target1, target2]\n",
        "train_imgs_no = 20000  # numarul de imagini de antrenament pentru fiecare dataset. Pentru seturile mai mici, se selecteaza automat un numar mai mic de imagini, dar se cicleaza in dataloader ca sa atinga nr asta de imagini\n",
        "val_imgs_no = train_imgs_no // 20  # numarul de imagini de validare sau de test pentru fiecare dataset\n",
        "loader_img_dim = 256  # dimensiunea imaginilor de input pentru dataloader (se redimensioneaza automat)\n",
        "batch_size = 1  # dimensiunea batch-ului pentru dataloader\n",
        "num_epochs = 15  # numarul de epoci pentru antrenare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sD-QqpDhkLy",
        "outputId": "a6494854-b0e6-4c22-9145-8c0d40fd596d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "if use_colab:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    !pip install --quiet gdown\n",
        "    import sys\n",
        "    sys.path.append(utils_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Vc5O85ph_GKs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "696f1efa-1816-4552-da61-f3005b4be8b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import time\n",
        "import numpy as np\n",
        "from utils.data_processing_utils import extract_and_store_data_from_datasets, get_model\n",
        "from utils.DomainDataset import DomainDataset, collate_fn\n",
        "from utils.UDAFasterRCNN import UDAFasterRCNN\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import SGD\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34hqt6RlTtha",
        "outputId": "af03042a-46d8-490d-fe7a-059c4e3fb36e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unzipped bmshare.zip successfully to /content/data\n",
            "Removed bmshare.zip successfully.\n"
          ]
        }
      ],
      "source": [
        "extract_and_store_data_from_datasets(data_dir, dataset_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0CuUMVKanzc2"
      },
      "outputs": [],
      "source": [
        "source_train_dir = os.path.join(data_dir, dataset_names[0], \"splitted_data/train\")\n",
        "source_val_dir = os.path.join(data_dir, dataset_names[0], \"splitted_data/val\")\n",
        "\n",
        "domain_labels = {'brats': 0, 'bmshare': 1, 'isles': 2}\n",
        "\n",
        "# Create datasets\n",
        "source_train_dataset = DomainDataset(source_train_dir, domain_label=domain_labels[dataset_names[0]], img_size=loader_img_dim, img_extension=\".png\", length=train_imgs_no)\n",
        "source_val_dataset   = DomainDataset(source_val_dir,   domain_label=domain_labels[dataset_names[0]], img_size=loader_img_dim, img_extension=\".png\", length=val_imgs_no)\n",
        "\n",
        "# Loaders\n",
        "source_train_loader = DataLoader(source_train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True, collate_fn=collate_fn)\n",
        "source_val_loader   = DataLoader(source_val_dataset,   batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RUMgPUg1smeS"
      },
      "outputs": [],
      "source": [
        "def get_lr(optimizer):\n",
        "    \"\"\"Get the current learning rate from the optimizer.\"\"\"\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "uibhl1FP15lU"
      },
      "outputs": [],
      "source": [
        "def train(source_train_loader, source_val_loader,\n",
        "             model, optimizer, scheduler, max_epochs, root_dir,\n",
        "             start_epoch=1, resume_training=False):\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    ckpt_path = os.path.join(root_dir, f\"New_Checkpoint_Simple_{dataset_names[0]}.pth\")\n",
        "    best_ckpt_path = os.path.join(root_dir, f\"New_Best_Checkpoint_Simple_{dataset_names[0]}.pth\")\n",
        "    empty_ckpt_history = {\n",
        "        'train_total_loss': [],\n",
        "        'val_total_loss': [],\n",
        "        'lr': [],\n",
        "        'train_epoch_time': [], 'val_epoch_time': [],\n",
        "    }\n",
        "\n",
        "\n",
        "    print('resume_training: ', resume_training)\n",
        "    if resume_training and os.path.exists(ckpt_path):\n",
        "        print(\"Loading checkpoint…\")\n",
        "        checkpoint = torch.load(ckpt_path, weights_only=False, map_location='cpu')\n",
        "        history = checkpoint.get('history', empty_ckpt_history)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        start_epoch = checkpoint['epoch'] + 1\n",
        "        best_val_loss = checkpoint['best_val_loss']\n",
        "        print(f\"Resuming training from epoch {start_epoch} with best_val_loss = {best_val_loss:.4f}\")\n",
        "    else:\n",
        "        history = empty_ckpt_history\n",
        "        print(\"Starting training from scratch\")\n",
        "\n",
        "    device = next(model.parameters()).device  # assume model is already on the right device\n",
        "\n",
        "    for epoch in range(start_epoch, max_epochs + 1):\n",
        "        global_step = 0\n",
        "        last_percent = -1\n",
        "        step = -1\n",
        "\n",
        "        train_total_loss = 0.0\n",
        "\n",
        "        print(f\"\\nEpoch {epoch}\")\n",
        "        print(\"Train:\", end=\"\", flush=True)\n",
        "\n",
        "        model.train()\n",
        "        epoch_train_start = time.time()\n",
        "\n",
        "        for batches in tqdm(source_train_loader):\n",
        "            step += 1\n",
        "            source_batch = batches\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Source batch\n",
        "            source_images, source_targets = source_batch\n",
        "            source_images = [img.to(device) for img in source_images]\n",
        "            source_targets = [{k: v.to(device) for k, v in t.items()} for t in source_targets]\n",
        "\n",
        "            # Forward\n",
        "            losses = model(source_images, source_targets)\n",
        "            total_loss = sum(losses.values())\n",
        "\n",
        "            # Backward + Optimizer step\n",
        "            total_loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            # Accumulate\n",
        "            train_total_loss += total_loss.item()\n",
        "            global_step += 1\n",
        "\n",
        "        # Normalize\n",
        "        num_train_steps = step + 1\n",
        "        train_total_loss /= num_train_steps\n",
        "\n",
        "        epoch_train_end = time.time()\n",
        "        elapsed1 = epoch_train_end - epoch_train_start\n",
        "        print(f\"\\nEpoch {epoch} training time: {int(elapsed1//60)}m {int(elapsed1%60)}s\")\n",
        "\n",
        "        # Validation\n",
        "        print(\"Val:\", end=\"\", flush=True)\n",
        "        # model.eval()\n",
        "        val_total_loss = 0.0\n",
        "        step = -1\n",
        "        last_percent = -1\n",
        "        epoch_val_start = time.time()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batches in tqdm(source_val_loader):\n",
        "                step += 1\n",
        "                source_batch = batches\n",
        "                # Source batch\n",
        "                source_images, source_targets = source_batch\n",
        "                source_images = [img.to(device) for img in source_images]\n",
        "                source_targets = [{k: v.to(device) for k, v in t.items()} for t in source_targets]\n",
        "\n",
        "                # Use the same last alpha or recompute if desired\n",
        "                losses = model(source_images, source_targets)\n",
        "                total_loss = sum(losses.values())\n",
        "\n",
        "                val_total_loss += total_loss.item()\n",
        "\n",
        "        # Normalize validation\n",
        "        num_val_steps = step + 1\n",
        "        val_total_loss /= num_val_steps\n",
        "\n",
        "        epoch_val_end = time.time()\n",
        "        elapsed2 = epoch_val_end - epoch_val_start\n",
        "        print(f\"\\nEpoch {epoch} validation time: {int(elapsed2//60)}m {int(elapsed2%60)}s\")\n",
        "\n",
        "        # Logging\n",
        "        print(f\"\\n[Epoch {epoch}]\")\n",
        "        print(f\"Train Loss: {train_total_loss:.4f}\")\n",
        "        print(f\" Val  Loss: {val_total_loss:.4f}\")\n",
        "\n",
        "        history['train_total_loss'].append(train_total_loss)\n",
        "        history['val_total_loss'].append(val_total_loss)\n",
        "        history['lr'].append(optimizer.param_groups[0]['lr'])\n",
        "        history['train_epoch_time'].append(elapsed1)\n",
        "        history['val_epoch_time'].append(elapsed2)\n",
        "\n",
        "        # Save best\n",
        "        if val_total_loss < best_val_loss:\n",
        "            print(\"Saving best model…\")\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'best_val_loss': best_val_loss,\n",
        "                'history': history\n",
        "            }, best_ckpt_path)\n",
        "            best_val_loss = val_total_loss\n",
        "\n",
        "        # Checkpoint\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'best_val_loss': best_val_loss,\n",
        "            'history': history\n",
        "        }, ckpt_path)\n",
        "\n",
        "        if scheduler is not None:\n",
        "            old_lr = get_lr(optimizer)\n",
        "            scheduler.step(val_total_loss)\n",
        "            new_lr = get_lr(optimizer)\n",
        "            if new_lr != old_lr:\n",
        "                print(f\"Learning rate changed from {old_lr:.6f} to {new_lr:.6f}\")\n",
        "\n",
        "    return\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSGeR1nw1-FL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6675d40f-da15-493b-ea1a-ae171b23e064"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resume_training:  False\n",
            "Starting training from scratch\n",
            "\n",
            "Epoch 1\n",
            "Train:"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 126/20000 [00:40<1:40:44,  3.29it/s]"
          ]
        }
      ],
      "source": [
        "NUM_CLASSES = 1 + len(dataset_names) # 3 + 1 (background)\n",
        "\n",
        "\n",
        "model = get_model(num_classes=2)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = SGD(\n",
        "    model.parameters(),\n",
        "    lr=1e-3,\n",
        "    momentum=0.9,\n",
        "    weight_decay=1e-4\n",
        ")\n",
        "scheduler = ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode='min',         # we want to minimize val loss\n",
        "    factor=0.5,         # lr ← lr * factor\n",
        "    patience=5,         # wait 5 epochs with no improvement\n",
        ")\n",
        "\n",
        "# Pass this into training:\n",
        "train(source_train_loader, source_val_loader,\n",
        "              model, optimizer, scheduler, max_epochs=num_epochs,\n",
        "              root_dir=ckpt_dir,\n",
        "              resume_training=resume_training\n",
        "         )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "uNyJTLpdR7Eh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}